What is RAGAS ?
ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. RAG denotes a class of LLM applications that use external data to augment the LLM’s context. There are existing tools and frameworks that help you build these pipelines but evaluating it and quantifying your pipeline performance can be hard. This is where ragas (RAG Assessment) comes in.

ragas provides you with the tools based on the latest research for evaluating LLM-generated text to give you insights about your RAG pipeline. ragas can be integrated with your CI/CD to provide continuous checks to ensure performance.

Ragas references the following data:
Question: These are the questions your RAG pipeline will be evaluated on.
Answer: The answer generated from the RAG pipeline and presented to the user.
Contexts: The contexts passed into the LLM to answer the question.
Ground Truths: The ground truth answer to the questions.

The harmonic mean of these 4 aspects gives you the ragas score which is a single measure of the performance of your QA system across all the important aspects.
The following output is produced by Ragas:
1. Retrieval: context_relevancy and context_recall which represents the measure of the performance of your retrieval system.

2. Generation : faithfulness which measures hallucinations and answer_relevancy which measures the answers to question relevance.

Explanation:
Faithfulness : how factually accurate is the generated answer
Aims to quantify hallucinations in generated answers: This point indicates that the evaluation process has the primary goal of measuring and quantifying hallucinations in the generated answers. Hallucinations refer to instances where the language model produces information or claims that are not accurate or supported by the input context. The evaluation seeks to identify and count these instances to gauge the model’s reliability and truthfulness.
Formulated as NLI problem: The evaluation approach is structured as a Natural Language Inference (NLI) problem. NLI involves determining the logical relationship between two statements: a premise and a hypothesis. In this context, the premise could be the context or input provided to the language model, while the hypothesis is the generated answer. By framing the evaluation in this way, it aims to assess whether the generated answer logically follows from the given context.
Identifies and verifies statements from the generated answer against the context: This point highlights that the evaluation process involves identifying individual statements within the generated answer and then verifying whether each statement is supported or corroborated by the context provided. This step helps assess the factual accuracy of the generated content and its alignment with the input context.
Final Score is then computed as a ratio of the number of statements that can be inferred to the total number of statements: To arrive at a final evaluation score, the process counts the number of statements within the generated answer that can be logically inferred or verified based on the input context. This count is then divided by the total number of statements in the answer. The resulting ratio provides a quantitative measure of the model’s ability to produce contextually relevant and accurate information. A higher ratio suggests better performance in terms of providing contextually relevant information.
Answer Relevancy : How relevant is the generated answer to the question
Aims to quantify presence of partial or redundant information in answer: This point refers to the goal of a particular evaluation or assessment process. When evaluating answers or responses generated by a system, one of the objectives is to determine the extent to which the response contains information that is partially repetitive or redundant. In other words, it seeks to measure whether the answer includes unnecessary or duplicated information that may affect its quality or effectiveness. By quantifying such redundancies or partial information, evaluators can provide feedback for improvement and ensure that responses are concise and informative.
Formulation of a QG problem: This point suggests the initiation of a Question Generation (QG) problem. In natural language processing, QG involves the creation of questions from given content or statements. Formulating a QG problem means defining the task of generating questions based on a specific context or text. This can be a crucial step in various applications, such as education, information retrieval, or content creation, as it enables the generation of questions that test comprehension, engage users, or aid in knowledge extraction.
Paradigm measures the similarity between generated question and the actual question: This point refers to an evaluation paradigm or method that assesses the similarity between questions generated by a system and questions that serve as a reference or ground truth (the “actual question”). In this context, the goal is to determine how closely the generated questions align with the desired or correct questions. Similarity measures can include various metrics, such as cosine similarity, Jaccard similarity, or more advanced natural language processing techniques. Assessing this similarity helps gauge the quality and relevance of the generated questions and provides insights into the performance of question generation systems.
Context Relevancy :
Aims to quantify precision of retrieved context : The evaluation process focuses on measuring how accurately the retrieved context aligns with the information needed to answer a question, providing a numerical representation of this precision.
Helps to optimize chunk size : This indicates that through evaluation, one can determine the ideal size or granularity of text chunks to be retrieved, ensuring that they are neither too large nor too small for effective information extraction.
Identifies and extracts sentences from given context taht is necessary to answer given question. : It refers to the evaluation’s role in pinpointing and extracting the specific sentences from the provided context that are relevant and essential for answering a given question accurately.
Final score :It is the ratio of number of extracted sentences to total number of sentences in the given context. The he evaluation’s ultimate metric is a ratio, quantifying the proportion of sentences correctly extracted from the context in relation to the total number of sentences in that context, offering a measure of the extraction’s completeness and accuracy.
Context Recall : Retrieving all the relevant context to answer the question
Requires annotated answer. This serves as a reference for assessing the system’s performance.
Formulated as a combination of candidate sentence extraction and NLI. This approach allows the system to estimate both the data points it correctly captured (True Positives, TP) and those it failed to capture (False Negatives, FN).The paradigm estimates the data points that were captured(TP) asnd also missed(FN)
Final Score = TP /(TP +FN). This score provides a quantitative measure of the system’s performance in capturing relevant information.